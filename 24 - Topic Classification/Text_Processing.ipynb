{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c31b197",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22829d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Sastrawi\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c05dacd",
   "metadata": {},
   "source": [
    "### Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f97be73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"\n",
    "Kecerdasan Buatan (AI) adalah bidang ilmu komputer yang dikhususkan memecahkan masalah kognitif yang umumnya\n",
    "terkait dengan kecerdasan manusia, seperti Learning, Problem Solving, dan Pattern Recognition.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6309db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "kecerdasan buatan (ai) adalah bidang ilmu komputer yang dikhususkan memecahkan masalah kognitif yang umumnya\n",
      "terkait dengan kecerdasan manusia, seperti learning, problem solving, dan pattern recognition.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lower_case = sentence.lower()\n",
    "print(lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8fd2a5",
   "metadata": {},
   "source": [
    "### Remove Unnecessary Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f7a93ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed7d8c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_sentence = \"\"\"\n",
    "Ada 5 algoritma machine learning popular yaitu Linear Regression, Logistic Regression, SVM, Decision Tree,\n",
    "dan Random Forest.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13d1b3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kecerdasan Buatan (AI) adalah bidang ilmu komputer yang dikhususkan memecahkan masalah kognitif yang umumnya\n",
      "terkait dengan kecerdasan manusia, seperti Learning, Problem Solving, dan Pattern Recognition.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = re.sub(r'\\d+', '', sentence)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6239037c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes! We have a match!\n"
     ]
    }
   ],
   "source": [
    "text = \"The rain in Spain\"\n",
    "result = re.search('^The.*Spain$', text)\n",
    "if result:\n",
    "    print('Yes! We have a match!')\n",
    "else:\n",
    "    print('No Match')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efcd03db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#nature', '#beautiful']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = \"wow!, it is a natural beauty. #nature #beautiful #\"\n",
    "\n",
    "x = re.findall('#[_]*[a-z]+', tweet)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e2991b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sensored sensored sensored sensored sensored sensored news'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = \"f**k f** fu*k fu** f**king f**king news\"\n",
    "\n",
    "x = re.sub('f[a-z]*\\*+[a-z]*', 'sensored', tweet)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7fd1610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['23 oct 2019']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date = \"23 oct 2019 23 oct, 2019 23 october, 2019, 2019 oct 26,2020\"\n",
    "x = re.findall('\\d{2} [a-z]{3} \\d{4}', date)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf987a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The cost of mobile is _'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The cost of mobile is $1000\"\n",
    "x = re.sub('\\$\\d+','_', text)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7780a058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'refer to '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"refer to http://medium.com\"\n",
    "x = re.sub('http[s]?\\://\\S+','', text)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8035058",
   "metadata": {},
   "source": [
    "### Remove Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b24d8b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \t AI adalah teknologi kunci\t\n"
     ]
    }
   ],
   "source": [
    "short_sentence = \" \\t AI adalah teknologi kunci\\t\"\n",
    "print(short_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78e42908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI adalah teknologi kunci\n"
     ]
    }
   ],
   "source": [
    "no_strip = short_sentence.strip()\n",
    "print(no_strip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b83415",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c1acc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46f07c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ada',\n",
       " '5',\n",
       " 'algoritma',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'popular',\n",
       " 'yaitu',\n",
       " 'Linear',\n",
       " 'Regression',\n",
       " ',',\n",
       " 'Logistic',\n",
       " 'Regression',\n",
       " ',',\n",
       " 'SVM',\n",
       " ',',\n",
       " 'Decision',\n",
       " 'Tree',\n",
       " ',',\n",
       " 'dan',\n",
       " 'Random',\n",
       " 'Forest',\n",
       " '.',\n",
       " \"''\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens = word_tokenize(long_sentence)\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc741444",
   "metadata": {},
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a0effde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ad908d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = StopWordRemoverFactory()\n",
    "stop_words = factory.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08d18b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2aca92bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('indonesian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef1d83f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentence = []\n",
    "for word_token in word_tokens:\n",
    "    if word_token not in stop_words:\n",
    "        filtered_sentence.append(word_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9752d6",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f02c4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eea5c7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d4dbae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indonesia bisa jadi bangsa berdikari jika adopsi teknologi ai dengan baik\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Indonesia bisa menjadi bangsa berdikari jika mengadopsi teknologi AI dengan baik'\n",
    "output = stemmer.stem(sentence)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0bbaf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "60f76f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming for studies is studi\n",
      "Stemming for studying is studi\n",
      "Stemming for cries is cri\n",
      "Stemming for cry is cri\n"
     ]
    }
   ],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "sentence = 'studies studying cries cry'\n",
    "tokenization = nltk.word_tokenize(sentence)\n",
    "\n",
    "for word in tokenization:\n",
    "    print('Stemming for {} is {}'.format(word, porter_stemmer.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e533856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58cc194a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for studies is study\n",
      "Lemma for studying is studying\n",
      "Lemma for cries is cry\n",
      "Lemma for cry is cry\n",
      "Lemma for crying is cry\n"
     ]
    }
   ],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "sentence = 'studies studying cries cry crying'\n",
    "tokenization = nltk.word_tokenize(sentence)\n",
    "for word in tokenization:\n",
    "    print('Lemma for {} is {}'.format(word, wordnet_lemmatizer.lemmatize(word)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
